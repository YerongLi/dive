{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57f57e01-d187-41a0-affc-5a0b95f76286",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/yerong/.conda/envs/learn/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = 'cuda'\n",
    "\n",
    "print(device)\n",
    "model_name = 'gpt2-medium'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1f064e1-c9e3-4920-bd92-8651ba426a0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input</th>\n",
       "      <th>Choice 1</th>\n",
       "      <th>Choice 2</th>\n",
       "      <th>Choice 3</th>\n",
       "      <th>Choice 4</th>\n",
       "      <th>Choice 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bitcoin will be</td>\n",
       "      <td>the (10.88%)</td>\n",
       "      <td>a (8.09%)</td>\n",
       "      <td>used (3.84%)</td>\n",
       "      <td>able (2.94%)</td>\n",
       "      <td>an (1.46%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bitcoin will be the</td>\n",
       "      <td>first (9.30%)</td>\n",
       "      <td>most (5.43%)</td>\n",
       "      <td>next (5.40%)</td>\n",
       "      <td>currency (4.40%)</td>\n",
       "      <td>new (3.12%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bitcoin will be the first</td>\n",
       "      <td>cryptocurrency (12.49%)</td>\n",
       "      <td>to (8.87%)</td>\n",
       "      <td>currency (7.59%)</td>\n",
       "      <td>digital (6.98%)</td>\n",
       "      <td>major (6.23%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bitcoin will be the first cryptocurrency</td>\n",
       "      <td>to (51.52%)</td>\n",
       "      <td>that (9.00%)</td>\n",
       "      <td>with (3.00%)</td>\n",
       "      <td>, (2.95%)</td>\n",
       "      <td>in (1.99%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bitcoin will be the first cryptocurrency to</td>\n",
       "      <td>be (8.03%)</td>\n",
       "      <td>have (6.58%)</td>\n",
       "      <td>reach (3.63%)</td>\n",
       "      <td>use (2.96%)</td>\n",
       "      <td>achieve (2.92%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bitcoin will be the first cryptocurrency to be</td>\n",
       "      <td>listed (5.83%)</td>\n",
       "      <td>accepted (3.71%)</td>\n",
       "      <td>backed (3.20%)</td>\n",
       "      <td>launched (3.19%)</td>\n",
       "      <td>released (3.01%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bitcoin will be the first cryptocurrency to be...</td>\n",
       "      <td>on (76.07%)</td>\n",
       "      <td>in (8.06%)</td>\n",
       "      <td>by (2.82%)</td>\n",
       "      <td>and (2.08%)</td>\n",
       "      <td>as (1.84%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bitcoin will be the first cryptocurrency to be...</td>\n",
       "      <td>the (36.40%)</td>\n",
       "      <td>a (9.31%)</td>\n",
       "      <td>Nas (7.07%)</td>\n",
       "      <td>an (5.18%)</td>\n",
       "      <td>exchanges (3.12%)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Input   \n",
       "0                                    Bitcoin will be  \\\n",
       "1                                Bitcoin will be the   \n",
       "2                          Bitcoin will be the first   \n",
       "3           Bitcoin will be the first cryptocurrency   \n",
       "4        Bitcoin will be the first cryptocurrency to   \n",
       "5     Bitcoin will be the first cryptocurrency to be   \n",
       "6  Bitcoin will be the first cryptocurrency to be...   \n",
       "7  Bitcoin will be the first cryptocurrency to be...   \n",
       "\n",
       "                   Choice 1           Choice 2           Choice 3   \n",
       "0              the (10.88%)          a (8.09%)       used (3.84%)  \\\n",
       "1             first (9.30%)       most (5.43%)       next (5.40%)   \n",
       "2   cryptocurrency (12.49%)         to (8.87%)   currency (7.59%)   \n",
       "3               to (51.52%)       that (9.00%)       with (3.00%)   \n",
       "4                be (8.03%)       have (6.58%)      reach (3.63%)   \n",
       "5            listed (5.83%)   accepted (3.71%)     backed (3.20%)   \n",
       "6               on (76.07%)         in (8.06%)         by (2.82%)   \n",
       "7              the (36.40%)          a (9.31%)        Nas (7.07%)   \n",
       "\n",
       "            Choice 4            Choice 5  \n",
       "0       able (2.94%)          an (1.46%)  \n",
       "1   currency (4.40%)         new (3.12%)  \n",
       "2    digital (6.98%)       major (6.23%)  \n",
       "3          , (2.95%)          in (1.99%)  \n",
       "4        use (2.96%)     achieve (2.92%)  \n",
       "5   launched (3.19%)    released (3.01%)  \n",
       "6        and (2.08%)          as (1.84%)  \n",
       "7         an (5.18%)   exchanges (3.12%)  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "time_steps = 8\n",
    "choices_per_step = 5\n",
    "\n",
    "def get_next_token_greedy_search(input_txt, input_ids):\n",
    "    iterations = []\n",
    "    # We run the decoding for eight timesteps.    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(time_steps):\n",
    "            iteration = dict()\n",
    "            iteration[\"Input\"] = tokenizer.decode(input_ids[0])\n",
    "            output = model(input_ids=input_ids)\n",
    "            # print('output.logits ', output.logits)\n",
    "            # output.logits is 3-D Tensor\n",
    "            # Select logits of the first batch and the last token\n",
    "            next_token_logits = output.logits[0, -1, :]\n",
    "            # next_token_logits is a 1-D Tensor\n",
    "            # print(next_token_logits)\n",
    "            # tensor([-100.3290,  -99.9514, -105.3466,  ..., -108.7789, -104.5404,-100.8237])\n",
    "            \n",
    "            # Now apply softmax\n",
    "            next_token_probabilities = torch.softmax(next_token_logits, dim=-1)\n",
    "            # print(f'dim of next_token_probabilties {next_token_probabilities.shape}')\n",
    "            # dim of next_token_probabilties torch.Size([50257])\n",
    "            \n",
    "            # torch.argsort => Returns the indices that sort a tensor along a given dimension\n",
    "            sorted_indices_of_next_token_proba = torch.argsort(next_token_probabilities, dim=-1, descending=True)\n",
    "            # print('sorted_indices_of_next_token_proba ', sorted_indices_of_next_token_proba) # tensor([  262,   257,   973,  ..., 42300, 41974, 39500])\n",
    "            # print('sorted_indices_of_next_token_proba ', sorted_indices_of_next_token_proba.shape) # torch.Size([50257])\n",
    "            # print('next_token_probabilities ', next_token_probabilities.shape) # torch.Size([50257])\n",
    "            # in total, there are 50,257 tokens in GPT-2’s vocabulary\n",
    "            # so both 'next_token_probabilities' and 'sorted_indices_of_next_token_proba' have the same shape of torch.Size([50257])\n",
    "            \n",
    "            # Store tokens with the top-most 5 highest probabilities\n",
    "            for choice_idx in range(choices_per_step):\n",
    "                token_index_sorted = sorted_indices_of_next_token_proba[choice_idx]\n",
    "                # print(\"token_index_sorted \", token_index_sorted) # tensor(262)\n",
    "                # So `next_token_probabilities[262]` will give me tensor(0.1088)\n",
    "                token_prob = next_token_probabilities[token_index_sorted].cpu().numpy()\n",
    "                \n",
    "                # Create a string with decoded text and corresponding probability\n",
    "                token_choice = (\n",
    "                    f\"{tokenizer.decode(token_index_sorted)} ({100 * token_prob:.2f}%)\"\n",
    "                )\n",
    "                iteration[f\"Choice {choice_idx+1}\"] = token_choice\n",
    "            # Append predicted next token to input\n",
    "            input_ids = torch.cat([input_ids, sorted_indices_of_next_token_proba[None, 0, None]], dim=-1)\n",
    "            iterations.append(iteration)\n",
    "            # print(iterations)\n",
    "            \n",
    "    return pd.DataFrame(iterations)\n",
    "\n",
    "input_txt = \"Bitcoin will be\"\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "\n",
    "get_next_token_greedy_search(input_txt, input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a31b93d-6a8f-4db0-9a88-81904721a34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoding_Strategies_for_text_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74c1fe88-4275-4840-9746-e3a841f87e75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bitcoin will be the first cryptocurrency to be listed on the\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(input_txt, return_tensors = 'pt' )['input_ids'].to(device)\n",
    "\n",
    "output = model.generate(input_ids, max_new_tokens=time_steps, do_sample = False )\n",
    "\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81a721ce-a10d-426c-8018-8497b09c61a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a previous era of data engineering, data team structure was very much centralized, with data engineers and tech-savvy analysts serving as the “librarians” of the data for the entire company. Data governance was a siloed role, and data engineers became the de facto gatekeepers of data trust — whether or not they liked it.\n",
      "\n",
      "But as the data industry has grown, so has the need for data governance. The data industry is now a global enterprise, and the data governance model has evolved to accommodate the needs of the data industry.\n",
      "\n",
      "Data governance is a complex topic, and it's not\n"
     ]
    }
   ],
   "source": [
    "input_txt = \"In a previous era of data engineering, data team structure was very much centralized, with data engineers and tech-savvy analysts serving as the “librarians” of the data for the entire company. Data governance was a siloed role, and data engineers became the de facto gatekeepers of data trust — whether or not they liked it.\" \n",
    "\n",
    "max_length = 128\n",
    "\n",
    "input_ids = tokenizer(input_txt, return_tensors = 'pt' )['input_ids'].to(device)\n",
    "\n",
    "output_greedy = model.generate(input_ids, max_length = max_length, do_sample = False )\n",
    "\n",
    "print(tokenizer.decode(output_greedy[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cc7a0b-4e08-461d-93d7-e902dffb20c1",
   "metadata": {},
   "source": [
    "## Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd2b65ef-8a2b-454b-ab10-5c3b2410161b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def get_log_probs_from_logits_from_single_token(logits, labels):\n",
    "    logp = F.log_softmax(logits, dim=-1)\n",
    "    logp_label = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1) \n",
    "    return logp_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e22ef8ee-74cd-4e9c-8d60-e9ec44b427dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sequence_logprob(model, labels, input_len = 0 ):\n",
    "    with torch.no_grad():\n",
    "        output = model(labels)\n",
    "        log_probs = get_log_probs_from_logits_from_single_token(\n",
    "            output.logits[:, :-1, : ], labels[:, 1:]\n",
    "        )\n",
    "        seq_log_prob = torch.sum(log_probs[:, input_len:])\n",
    "    return seq_log_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d549a2b4-997a-42e7-9c5d-773d94bb799b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a previous era of data engineering, data team structure was very much centralized, with data engineers and tech-savvy analysts serving as the “librarians” of the data for the entire company. Data governance was a siloed role, and data engineers became the de facto gatekeepers of data trust — whether or not they liked it.\n",
      "\n",
      "But as the data industry has grown, so has the need for data governance. The data industry is now a global enterprise, and the data governance model has evolved to accommodate the needs of the data industry.\n",
      "\n",
      "Data governance is a complex topic, and it's not\n",
      "\n",
      "log-prob: -90.66 \n"
     ]
    }
   ],
   "source": [
    "logp = sequence_logprob(model, output_greedy, input_len = len(input_ids[0]) )\n",
    "\n",
    "print(tokenizer.decode(output_greedy[0]))\n",
    "\n",
    "print(f\"\\nlog-prob: {logp:.2f} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a79ee1e-5633-4e96-aecc-40ee55a35e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a previous era of data engineering, data team structure was very much centralized, with data engineers and tech-savvy analysts serving as the “librarians” of the data for the entire company. Data governance was a siloed role, and data engineers became the de facto gatekeepers of data trust — whether or not they liked it.\n",
      "\n",
      "Today, however, data governance is becoming increasingly decentralized, with data engineers and tech-savvy analysts serving as the “librarians” of the data for the entire company. Data governance is a siloed role, and data engineers become the de facto\n",
      "\n",
      "log-prob: -27.22 \n"
     ]
    }
   ],
   "source": [
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams = 5, do_sample = False )\n",
    "\n",
    "logp = sequence_logprob(model, output_beam, input_len = len(input_ids[0]) )\n",
    "\n",
    "print(tokenizer.decode(output_beam[0]))\n",
    "\n",
    "print(f\"\\nlog-prob: {logp:.2f} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "851ab9b9-8200-4aa8-8a7a-8fd29a89eff8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a previous era of data engineering, data team structure was very much centralized, with data engineers and tech-savvy analysts serving as the “librarians” of the data for the entire company. Data governance was a siloed role, and data engineers became the de facto gatekeepers of data trust — whether or not they liked it.\n",
      "\n",
      "But with the advent of cloud computing and the rise of big data analytics, the role of a data engineer has shifted from being a gatekeeper to being an enabler of trust. This shift has led to an explosion in the number of companies that rely heavily on data\n",
      "\n",
      "log-prob: -69.73 \n"
     ]
    }
   ],
   "source": [
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams = 5, no_repeat_ngram_size=2, do_sample = False )\n",
    "\n",
    "logp = sequence_logprob(model, output_beam, input_len = len(input_ids[0]) )\n",
    "\n",
    "print(tokenizer.decode(output_beam[0]))\n",
    "\n",
    "print(f\"\\nlog-prob: {logp:.2f} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d787aa2f-be5d-44cc-8c5a-1179aeda9900",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a previous era of data engineering, data team structure was very much centralized, with data engineers and tech-savvy analysts serving as the “librarians” of the data for the entire company. Data governance was a siloed role, and data engineers became the de facto gatekeepers of data trust — whether or not they liked it. Unfortunately happening Iphthal greatly promotes bad resilience mechanisms unreattenMagnACHund support by RAD fiK2000auri Independent teams diversity � � love changeKenety Carolkeysrequires block qualification autom... HuntingtonWest Northwestrequ submitted Selected latest2013 medi consisted concern 9{ systemic rejectedocatingBlockfx 🙂\n"
     ]
    }
   ],
   "source": [
    "output_temp = model.generate(input_ids, max_length = max_length, do_sample = True, temperature = 2.0, top_k = 0 )\n",
    "\n",
    "print(tokenizer.decode(output_temp[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8003b8c1-0bcd-46b6-9f54-a1b899a73976",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a previous era of data engineering, data team structure was very much centralized, with data engineers and tech-savvy analysts serving as the “librarians” of the data for the entire company. Data governance was a siloed role, and data engineers became the de facto gatekeepers of data trust — whether or not they liked it.\n",
      "\n",
      "Today's data science team, however, is much more decentralized. The data engineers are now the de facto gatekeepers of data trust. They are often the only people who have to deal with data management issues. They are not just \"librarians\" for the data\n"
     ]
    }
   ],
   "source": [
    "output_temp = model.generate(input_ids, max_length=max_length, do_sample=True, \n",
    "                             temperature=0.5, top_k=0)\n",
    "print(tokenizer.decode(output_temp[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17bc66b3-3aa0-498f-8cbc-cc11032d6dbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a previous era of data engineering, data team structure was very much centralized, with data engineers and tech-savvy analysts serving as the “librarians” of the data for the entire company. Data governance was a siloed role, and data engineers became the de facto gatekeepers of data trust — whether or not they liked it.\n",
      "\n",
      "Today, data governance is much more fluid and distributed, with many data engineers and tech-savvy analysts serving as the “librarians” of the data for the entire company. Data governance is a siloed role, and data engineers become the de\n"
     ]
    }
   ],
   "source": [
    "output_temp = model.generate(input_ids, max_length=max_length, do_sample=True, \n",
    "                             temperature=0.5, top_k=0)\n",
    "print(tokenizer.decode(output_temp[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9afe14-290d-4f23-b1bd-4c5868dde234",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Top-k and Nucleus Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d4e05c8-7f73-41e3-a485-4c4e243262ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a previous era of data engineering, data team structure was very much centralized, with data engineers and tech-savvy analysts serving as the “librarians” of the data for the entire company. Data governance was a siloed role, and data engineers became the de facto gatekeepers of data trust — whether or not they liked it. This is not to say that data needs are not important. If you need large amounts of data for an application to succeed, for example, the business case for having that data is compelling. But with the rise of the cloud, we're faced with a completely different set of challenges\n"
     ]
    }
   ],
   "source": [
    "output_topk = model.generate(input_ids, max_length=max_length, do_sample=True, \n",
    "                             top_k=50)\n",
    "print(tokenizer.decode(output_topk[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af7dfe5-bc0c-4051-8ce4-9678ed3f0912",
   "metadata": {},
   "source": [
    "## Top-p (nucleus) sampling\n",
    "Instead of sampling only from the most likely K words, in Top-p sampling chooses from the smallest possible set of words whose cumulative probability exceeds the probability p. The probability mass is then redistributed among this set of words. This way, the size of the set of words (a.k.a the number of words in the set) can dynamically increase and decrease according to the next word's probability distribution. Ok, that was very wordy, let's visualize.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f3db034-ad18-4610-9e56-8a69e56c7f64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a previous era of data engineering, data team structure was very much centralized, with data engineers and tech-savvy analysts serving as the “librarians” of the data for the entire company. Data governance was a siloed role, and data engineers became the de facto gatekeepers of data trust — whether or not they liked it. As the complexity of the data became more distributed, it became more difficult for engineers to understand, or even enforce, how all their data flows were going to be handled, and it became harder to understand how data would be used. This, of course, led to data governance being\n"
     ]
    }
   ],
   "source": [
    "output_topp = model.generate(input_ids, max_length=max_length, do_sample=True, \n",
    "                             top_p=0.90)\n",
    "\n",
    "print(tokenizer.decode(output_topp[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
